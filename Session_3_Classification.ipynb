{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Session 3 - Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4bSnGjcYEgH"
      },
      "source": [
        "Uploading Dataset to Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsJ3I6v4XYAX",
        "outputId": "596a9918-c75d-4666-d70e-ff17ae7bc511",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-10bec985-be8f-4e9a-81b4-43ae57b56958\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-10bec985-be8f-4e9a-81b4-43ae57b56958\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving iris.csv to iris.csv\n",
            "User uploaded file \"iris.csv\" with length 4698 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuh280IYYQGm"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3--rCz3XYA0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GXKTpw-YqGn"
      },
      "source": [
        "Example of Data Loader Class Code for Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLBkRNABXYBe"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class IrisDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = torch.Tensor(self.X[index])\n",
        "    y = torch.LongTensor(self.y[index, None])\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKAmPcuTZh45"
      },
      "source": [
        "Loading and Preprocessing Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79dDaZucXYBy",
        "outputId": "acd5591c-e25a-4a88-e471-ef3d3182e984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# open dataset from csv\n",
        "dataset = pd.read_csv('iris.csv',names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\",\"species\"])\n",
        "print(dataset)\n",
        "\n",
        "# transform labels to numerics\n",
        "dataset.loc[dataset.species=='Iris-setosa', 'species'] = 0\n",
        "dataset.loc[dataset.species=='Iris-versicolor', 'species'] = 1\n",
        "dataset.loc[dataset.species=='Iris-virginica', 'species'] = 2\n",
        "\n",
        "# get the features and labels from the dataset\n",
        "X = dataset[dataset.columns[0:4]].values\n",
        "y = dataset.species.values.astype(np.int64)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     sepal_length  sepal_width  petal_length  petal_width         species\n",
            "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
            "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
            "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
            "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
            "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
            "..            ...          ...           ...          ...             ...\n",
            "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
            "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
            "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
            "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
            "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
            "\n",
            "[150 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSkLwnH_ZXxM"
      },
      "source": [
        "Preprocessing Iris Dataset with Z-Score Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L4gNcM8ZSTU"
      },
      "source": [
        "# preprocessing with z-score normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Do2cblZwnx"
      },
      "source": [
        "Training, Validation and Testing Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAQJgveRXYB8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, \n",
        "                                                      test_size=0.2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjGN7FoVZ7Fz"
      },
      "source": [
        "Data Loader Class Instantiation for Training, Validation and Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-N543q1Z70S"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = IrisDataset(train_X, train_y)\n",
        "train_loader = DataLoader(train_ds, batch_size=16, \n",
        "                             shuffle=True, num_workers=0)\n",
        "\n",
        "valid_ds = IrisDataset(valid_X, valid_y)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=16, \n",
        "                             shuffle=False, num_workers=0)\n",
        "\n",
        "test_ds = IrisDataset(test_X, test_y)\n",
        "test_loader = DataLoader(test_ds, batch_size=16, \n",
        "                            shuffle=False, num_workers=0)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H07aOBzBaH1R"
      },
      "source": [
        "Constructing the Deep Learning Model\n",
        "\n",
        "Input -> Fully Connected 1 -> ReLU -> Batch Normalization 1 -> Fully Connected 2 -> ReLU -> Batch Normalization 2 -> Fully Connected 3 -> Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74zdBiEYXYCG"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    # define nn\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 100)\n",
        "        self.bn1 = nn.BatchNorm1d(100)\n",
        "        \n",
        "        self.fc2 = nn.Linear(100, 100)\n",
        "        self.bn2 = nn.BatchNorm1d(100)\n",
        "        \n",
        "        self.fc3 = nn.Linear(100, 3)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.fc1(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.bn1(X)\n",
        "        X = self.fc2(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.bn2(X)\n",
        "        X = self.fc3(X)\n",
        "\n",
        "        return X"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tncpr8iWaZ2w"
      },
      "source": [
        "Instantiating the Model, Choosing the Loss Function, and Choosing the Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kDyVywcXYCM"
      },
      "source": [
        "# Instantiating the model\n",
        "net = Net()\n",
        "\n",
        "# Choosing the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Choosing the optimizer\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# alternatif optimizer: Adam\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVdEfMyXamWh"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmJDHBzCXYCZ",
        "outputId": "ebe5cb1a-38a8-4dc6-d3fa-c0f76ccea67c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 300\n",
        " \n",
        "train_mean_losses = []\n",
        "valid_mean_losses = []\n",
        "\n",
        "valid_best_loss = np.inf\n",
        "\n",
        "for i in range(epochs):  \n",
        "    #===============================================================\n",
        "    # training \n",
        "    train_losses = []\n",
        "    \n",
        "    print(\"=========================================================\")\n",
        "    print(\"Epoch {}\".format(i))\n",
        "    \n",
        "    for iteration, batch_data in enumerate(train_loader):\n",
        "        X_batch, y_batch = batch_data\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        out = net(X_batch)\n",
        "        loss = criterion(out, y_batch.squeeze())\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_losses.append(loss)\n",
        "    \n",
        "    train_mean_loss = torch.mean(torch.stack(train_losses))\n",
        "    print('training loss: {:10.8f}'.format(train_mean_loss))\n",
        "    \n",
        "    train_mean_losses.append(train_mean_loss)\n",
        "    \n",
        "    #===============================================================\n",
        "    # validation\n",
        "    valid_losses = []\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for iteration, batch_data in enumerate(valid_loader):\n",
        "            X_batch, y_batch = batch_data\n",
        "\n",
        "            out = net(X_batch)\n",
        "            loss = criterion(out, y_batch.squeeze())\n",
        "            valid_losses.append(loss)\n",
        "            \n",
        "        valid_mean_loss = torch.mean(torch.stack(valid_losses))\n",
        "        print('validation loss: {:10.8f}'.format(valid_mean_loss))\n",
        "        \n",
        "        valid_mean_losses.append(valid_mean_loss)\n",
        "        \n",
        "        if valid_mean_loss.cpu().numpy()[()] < valid_best_loss:\n",
        "            valid_best_loss = valid_mean_loss\n",
        "            torch.save(net.state_dict(), \"best_model.pth\")\n",
        "            best_epoch = i\n",
        "    #===============================================================\n",
        "            "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================\n",
            "Epoch 0\n",
            "training loss: 0.80736303\n",
            "validation loss: 0.84617758\n",
            "=========================================================\n",
            "Epoch 1\n",
            "training loss: 0.46600854\n",
            "validation loss: 0.74723887\n",
            "=========================================================\n",
            "Epoch 2\n",
            "training loss: 0.37317660\n",
            "validation loss: 0.70108366\n",
            "=========================================================\n",
            "Epoch 3\n",
            "training loss: 0.27536768\n",
            "validation loss: 0.67018539\n",
            "=========================================================\n",
            "Epoch 4\n",
            "training loss: 0.22840048\n",
            "validation loss: 0.65225875\n",
            "=========================================================\n",
            "Epoch 5\n",
            "training loss: 0.27726048\n",
            "validation loss: 0.63868380\n",
            "=========================================================\n",
            "Epoch 6\n",
            "training loss: 0.21232374\n",
            "validation loss: 0.62267369\n",
            "=========================================================\n",
            "Epoch 7\n",
            "training loss: 0.19420864\n",
            "validation loss: 0.62152672\n",
            "=========================================================\n",
            "Epoch 8\n",
            "training loss: 0.20862277\n",
            "validation loss: 0.61146331\n",
            "=========================================================\n",
            "Epoch 9\n",
            "training loss: 0.17152268\n",
            "validation loss: 0.60237759\n",
            "=========================================================\n",
            "Epoch 10\n",
            "training loss: 0.17234616\n",
            "validation loss: 0.58996636\n",
            "=========================================================\n",
            "Epoch 11\n",
            "training loss: 0.14343502\n",
            "validation loss: 0.58892226\n",
            "=========================================================\n",
            "Epoch 12\n",
            "training loss: 0.16642813\n",
            "validation loss: 0.59169436\n",
            "=========================================================\n",
            "Epoch 13\n",
            "training loss: 0.17074351\n",
            "validation loss: 0.59157652\n",
            "=========================================================\n",
            "Epoch 14\n",
            "training loss: 0.13353923\n",
            "validation loss: 0.59437680\n",
            "=========================================================\n",
            "Epoch 15\n",
            "training loss: 0.12684299\n",
            "validation loss: 0.59970778\n",
            "=========================================================\n",
            "Epoch 16\n",
            "training loss: 0.11019859\n",
            "validation loss: 0.60087013\n",
            "=========================================================\n",
            "Epoch 17\n",
            "training loss: 0.10680553\n",
            "validation loss: 0.60690093\n",
            "=========================================================\n",
            "Epoch 18\n",
            "training loss: 0.18492855\n",
            "validation loss: 0.61216229\n",
            "=========================================================\n",
            "Epoch 19\n",
            "training loss: 0.09322993\n",
            "validation loss: 0.61644459\n",
            "=========================================================\n",
            "Epoch 20\n",
            "training loss: 0.11070462\n",
            "validation loss: 0.61672145\n",
            "=========================================================\n",
            "Epoch 21\n",
            "training loss: 0.08331937\n",
            "validation loss: 0.61733520\n",
            "=========================================================\n",
            "Epoch 22\n",
            "training loss: 0.09702746\n",
            "validation loss: 0.61716276\n",
            "=========================================================\n",
            "Epoch 23\n",
            "training loss: 0.18198113\n",
            "validation loss: 0.62409753\n",
            "=========================================================\n",
            "Epoch 24\n",
            "training loss: 0.13455747\n",
            "validation loss: 0.62538707\n",
            "=========================================================\n",
            "Epoch 25\n",
            "training loss: 0.09204004\n",
            "validation loss: 0.62925029\n",
            "=========================================================\n",
            "Epoch 26\n",
            "training loss: 0.09099668\n",
            "validation loss: 0.62739772\n",
            "=========================================================\n",
            "Epoch 27\n",
            "training loss: 0.10761879\n",
            "validation loss: 0.63244992\n",
            "=========================================================\n",
            "Epoch 28\n",
            "training loss: 0.13868652\n",
            "validation loss: 0.63850808\n",
            "=========================================================\n",
            "Epoch 29\n",
            "training loss: 0.05812884\n",
            "validation loss: 0.64022166\n",
            "=========================================================\n",
            "Epoch 30\n",
            "training loss: 0.10855943\n",
            "validation loss: 0.64166588\n",
            "=========================================================\n",
            "Epoch 31\n",
            "training loss: 0.10069844\n",
            "validation loss: 0.63649285\n",
            "=========================================================\n",
            "Epoch 32\n",
            "training loss: 0.09266478\n",
            "validation loss: 0.63906109\n",
            "=========================================================\n",
            "Epoch 33\n",
            "training loss: 0.10763098\n",
            "validation loss: 0.63727850\n",
            "=========================================================\n",
            "Epoch 34\n",
            "training loss: 0.06246494\n",
            "validation loss: 0.64095569\n",
            "=========================================================\n",
            "Epoch 35\n",
            "training loss: 0.05741232\n",
            "validation loss: 0.64018953\n",
            "=========================================================\n",
            "Epoch 36\n",
            "training loss: 0.12466636\n",
            "validation loss: 0.65063673\n",
            "=========================================================\n",
            "Epoch 37\n",
            "training loss: 0.07482544\n",
            "validation loss: 0.65502489\n",
            "=========================================================\n",
            "Epoch 38\n",
            "training loss: 0.08926026\n",
            "validation loss: 0.64858860\n",
            "=========================================================\n",
            "Epoch 39\n",
            "training loss: 0.09996775\n",
            "validation loss: 0.65160942\n",
            "=========================================================\n",
            "Epoch 40\n",
            "training loss: 0.08857774\n",
            "validation loss: 0.65282691\n",
            "=========================================================\n",
            "Epoch 41\n",
            "training loss: 0.08098701\n",
            "validation loss: 0.65000111\n",
            "=========================================================\n",
            "Epoch 42\n",
            "training loss: 0.12223087\n",
            "validation loss: 0.64311630\n",
            "=========================================================\n",
            "Epoch 43\n",
            "training loss: 0.05048129\n",
            "validation loss: 0.64136291\n",
            "=========================================================\n",
            "Epoch 44\n",
            "training loss: 0.06946256\n",
            "validation loss: 0.64778250\n",
            "=========================================================\n",
            "Epoch 45\n",
            "training loss: 0.06895324\n",
            "validation loss: 0.65047240\n",
            "=========================================================\n",
            "Epoch 46\n",
            "training loss: 0.07116872\n",
            "validation loss: 0.64828724\n",
            "=========================================================\n",
            "Epoch 47\n",
            "training loss: 0.11675984\n",
            "validation loss: 0.65243489\n",
            "=========================================================\n",
            "Epoch 48\n",
            "training loss: 0.06395454\n",
            "validation loss: 0.65599287\n",
            "=========================================================\n",
            "Epoch 49\n",
            "training loss: 0.05367349\n",
            "validation loss: 0.64213657\n",
            "=========================================================\n",
            "Epoch 50\n",
            "training loss: 0.06250343\n",
            "validation loss: 0.64384168\n",
            "=========================================================\n",
            "Epoch 51\n",
            "training loss: 0.06696755\n",
            "validation loss: 0.64598185\n",
            "=========================================================\n",
            "Epoch 52\n",
            "training loss: 0.05620186\n",
            "validation loss: 0.65048015\n",
            "=========================================================\n",
            "Epoch 53\n",
            "training loss: 0.05426787\n",
            "validation loss: 0.65250605\n",
            "=========================================================\n",
            "Epoch 54\n",
            "training loss: 0.05350680\n",
            "validation loss: 0.65451217\n",
            "=========================================================\n",
            "Epoch 55\n",
            "training loss: 0.15180467\n",
            "validation loss: 0.65218002\n",
            "=========================================================\n",
            "Epoch 56\n",
            "training loss: 0.07301580\n",
            "validation loss: 0.65804964\n",
            "=========================================================\n",
            "Epoch 57\n",
            "training loss: 0.06537797\n",
            "validation loss: 0.65751404\n",
            "=========================================================\n",
            "Epoch 58\n",
            "training loss: 0.03944331\n",
            "validation loss: 0.65809166\n",
            "=========================================================\n",
            "Epoch 59\n",
            "training loss: 0.02578811\n",
            "validation loss: 0.65797353\n",
            "=========================================================\n",
            "Epoch 60\n",
            "training loss: 0.03183816\n",
            "validation loss: 0.65871352\n",
            "=========================================================\n",
            "Epoch 61\n",
            "training loss: 0.04417171\n",
            "validation loss: 0.66183060\n",
            "=========================================================\n",
            "Epoch 62\n",
            "training loss: 0.04440042\n",
            "validation loss: 0.65719938\n",
            "=========================================================\n",
            "Epoch 63\n",
            "training loss: 0.11427855\n",
            "validation loss: 0.66040492\n",
            "=========================================================\n",
            "Epoch 64\n",
            "training loss: 0.02692075\n",
            "validation loss: 0.65898645\n",
            "=========================================================\n",
            "Epoch 65\n",
            "training loss: 0.05642824\n",
            "validation loss: 0.66116983\n",
            "=========================================================\n",
            "Epoch 66\n",
            "training loss: 0.06646857\n",
            "validation loss: 0.66861886\n",
            "=========================================================\n",
            "Epoch 67\n",
            "training loss: 0.11104675\n",
            "validation loss: 0.67185211\n",
            "=========================================================\n",
            "Epoch 68\n",
            "training loss: 0.07552274\n",
            "validation loss: 0.67822856\n",
            "=========================================================\n",
            "Epoch 69\n",
            "training loss: 0.03255231\n",
            "validation loss: 0.67917013\n",
            "=========================================================\n",
            "Epoch 70\n",
            "training loss: 0.06181949\n",
            "validation loss: 0.68124783\n",
            "=========================================================\n",
            "Epoch 71\n",
            "training loss: 0.04285303\n",
            "validation loss: 0.68225622\n",
            "=========================================================\n",
            "Epoch 72\n",
            "training loss: 0.07516124\n",
            "validation loss: 0.68138707\n",
            "=========================================================\n",
            "Epoch 73\n",
            "training loss: 0.03970169\n",
            "validation loss: 0.68203974\n",
            "=========================================================\n",
            "Epoch 74\n",
            "training loss: 0.05809134\n",
            "validation loss: 0.68167627\n",
            "=========================================================\n",
            "Epoch 75\n",
            "training loss: 0.05105146\n",
            "validation loss: 0.68889469\n",
            "=========================================================\n",
            "Epoch 76\n",
            "training loss: 0.05140867\n",
            "validation loss: 0.69279337\n",
            "=========================================================\n",
            "Epoch 77\n",
            "training loss: 0.05221701\n",
            "validation loss: 0.69712293\n",
            "=========================================================\n",
            "Epoch 78\n",
            "training loss: 0.04011762\n",
            "validation loss: 0.69142401\n",
            "=========================================================\n",
            "Epoch 79\n",
            "training loss: 0.04841824\n",
            "validation loss: 0.69319141\n",
            "=========================================================\n",
            "Epoch 80\n",
            "training loss: 0.02282392\n",
            "validation loss: 0.69371819\n",
            "=========================================================\n",
            "Epoch 81\n",
            "training loss: 0.10136161\n",
            "validation loss: 0.69374430\n",
            "=========================================================\n",
            "Epoch 82\n",
            "training loss: 0.06120020\n",
            "validation loss: 0.68903542\n",
            "=========================================================\n",
            "Epoch 83\n",
            "training loss: 0.06211638\n",
            "validation loss: 0.69448835\n",
            "=========================================================\n",
            "Epoch 84\n",
            "training loss: 0.06094603\n",
            "validation loss: 0.70080805\n",
            "=========================================================\n",
            "Epoch 85\n",
            "training loss: 0.04191506\n",
            "validation loss: 0.70287603\n",
            "=========================================================\n",
            "Epoch 86\n",
            "training loss: 0.06178904\n",
            "validation loss: 0.70439702\n",
            "=========================================================\n",
            "Epoch 87\n",
            "training loss: 0.06408706\n",
            "validation loss: 0.71667618\n",
            "=========================================================\n",
            "Epoch 88\n",
            "training loss: 0.02495377\n",
            "validation loss: 0.71212059\n",
            "=========================================================\n",
            "Epoch 89\n",
            "training loss: 0.03189405\n",
            "validation loss: 0.71397471\n",
            "=========================================================\n",
            "Epoch 90\n",
            "training loss: 0.06850008\n",
            "validation loss: 0.70892894\n",
            "=========================================================\n",
            "Epoch 91\n",
            "training loss: 0.02394389\n",
            "validation loss: 0.71153516\n",
            "=========================================================\n",
            "Epoch 92\n",
            "training loss: 0.03663239\n",
            "validation loss: 0.70934319\n",
            "=========================================================\n",
            "Epoch 93\n",
            "training loss: 0.07994427\n",
            "validation loss: 0.71488196\n",
            "=========================================================\n",
            "Epoch 94\n",
            "training loss: 0.06770105\n",
            "validation loss: 0.70756245\n",
            "=========================================================\n",
            "Epoch 95\n",
            "training loss: 0.04406865\n",
            "validation loss: 0.70914543\n",
            "=========================================================\n",
            "Epoch 96\n",
            "training loss: 0.05077263\n",
            "validation loss: 0.71199358\n",
            "=========================================================\n",
            "Epoch 97\n",
            "training loss: 0.14559454\n",
            "validation loss: 0.68929416\n",
            "=========================================================\n",
            "Epoch 98\n",
            "training loss: 0.09337860\n",
            "validation loss: 0.70901150\n",
            "=========================================================\n",
            "Epoch 99\n",
            "training loss: 0.03870826\n",
            "validation loss: 0.70243979\n",
            "=========================================================\n",
            "Epoch 100\n",
            "training loss: 0.02484241\n",
            "validation loss: 0.71020877\n",
            "=========================================================\n",
            "Epoch 101\n",
            "training loss: 0.03841209\n",
            "validation loss: 0.71163118\n",
            "=========================================================\n",
            "Epoch 102\n",
            "training loss: 0.04658917\n",
            "validation loss: 0.71788096\n",
            "=========================================================\n",
            "Epoch 103\n",
            "training loss: 0.03405543\n",
            "validation loss: 0.72378963\n",
            "=========================================================\n",
            "Epoch 104\n",
            "training loss: 0.02522546\n",
            "validation loss: 0.72428155\n",
            "=========================================================\n",
            "Epoch 105\n",
            "training loss: 0.05022301\n",
            "validation loss: 0.72785491\n",
            "=========================================================\n",
            "Epoch 106\n",
            "training loss: 0.02772718\n",
            "validation loss: 0.72451788\n",
            "=========================================================\n",
            "Epoch 107\n",
            "training loss: 0.02848072\n",
            "validation loss: 0.72581130\n",
            "=========================================================\n",
            "Epoch 108\n",
            "training loss: 0.06237905\n",
            "validation loss: 0.72859889\n",
            "=========================================================\n",
            "Epoch 109\n",
            "training loss: 0.03293741\n",
            "validation loss: 0.72792500\n",
            "=========================================================\n",
            "Epoch 110\n",
            "training loss: 0.06082134\n",
            "validation loss: 0.72134411\n",
            "=========================================================\n",
            "Epoch 111\n",
            "training loss: 0.02465259\n",
            "validation loss: 0.72632277\n",
            "=========================================================\n",
            "Epoch 112\n",
            "training loss: 0.04073834\n",
            "validation loss: 0.72687113\n",
            "=========================================================\n",
            "Epoch 113\n",
            "training loss: 0.08501408\n",
            "validation loss: 0.73301679\n",
            "=========================================================\n",
            "Epoch 114\n",
            "training loss: 0.01784215\n",
            "validation loss: 0.73152691\n",
            "=========================================================\n",
            "Epoch 115\n",
            "training loss: 0.02962736\n",
            "validation loss: 0.73534465\n",
            "=========================================================\n",
            "Epoch 116\n",
            "training loss: 0.06929418\n",
            "validation loss: 0.72184730\n",
            "=========================================================\n",
            "Epoch 117\n",
            "training loss: 0.04805435\n",
            "validation loss: 0.73524326\n",
            "=========================================================\n",
            "Epoch 118\n",
            "training loss: 0.07509031\n",
            "validation loss: 0.72121584\n",
            "=========================================================\n",
            "Epoch 119\n",
            "training loss: 0.14062460\n",
            "validation loss: 0.74606240\n",
            "=========================================================\n",
            "Epoch 120\n",
            "training loss: 0.02467665\n",
            "validation loss: 0.74027193\n",
            "=========================================================\n",
            "Epoch 121\n",
            "training loss: 0.13432406\n",
            "validation loss: 0.72823542\n",
            "=========================================================\n",
            "Epoch 122\n",
            "training loss: 0.07794095\n",
            "validation loss: 0.72336942\n",
            "=========================================================\n",
            "Epoch 123\n",
            "training loss: 0.03345071\n",
            "validation loss: 0.72430921\n",
            "=========================================================\n",
            "Epoch 124\n",
            "training loss: 0.06552403\n",
            "validation loss: 0.72302151\n",
            "=========================================================\n",
            "Epoch 125\n",
            "training loss: 0.06143849\n",
            "validation loss: 0.73082328\n",
            "=========================================================\n",
            "Epoch 126\n",
            "training loss: 0.04882992\n",
            "validation loss: 0.72992641\n",
            "=========================================================\n",
            "Epoch 127\n",
            "training loss: 0.02788140\n",
            "validation loss: 0.73103261\n",
            "=========================================================\n",
            "Epoch 128\n",
            "training loss: 0.03895318\n",
            "validation loss: 0.73164755\n",
            "=========================================================\n",
            "Epoch 129\n",
            "training loss: 0.04774003\n",
            "validation loss: 0.73336166\n",
            "=========================================================\n",
            "Epoch 130\n",
            "training loss: 0.02867918\n",
            "validation loss: 0.73185706\n",
            "=========================================================\n",
            "Epoch 131\n",
            "training loss: 0.02462755\n",
            "validation loss: 0.73539340\n",
            "=========================================================\n",
            "Epoch 132\n",
            "training loss: 0.02362033\n",
            "validation loss: 0.73992944\n",
            "=========================================================\n",
            "Epoch 133\n",
            "training loss: 0.03483595\n",
            "validation loss: 0.74334401\n",
            "=========================================================\n",
            "Epoch 134\n",
            "training loss: 0.08682481\n",
            "validation loss: 0.75424647\n",
            "=========================================================\n",
            "Epoch 135\n",
            "training loss: 0.04498005\n",
            "validation loss: 0.75412977\n",
            "=========================================================\n",
            "Epoch 136\n",
            "training loss: 0.04620996\n",
            "validation loss: 0.75230086\n",
            "=========================================================\n",
            "Epoch 137\n",
            "training loss: 0.04941137\n",
            "validation loss: 0.74231523\n",
            "=========================================================\n",
            "Epoch 138\n",
            "training loss: 0.01937698\n",
            "validation loss: 0.74055457\n",
            "=========================================================\n",
            "Epoch 139\n",
            "training loss: 0.01511983\n",
            "validation loss: 0.73950022\n",
            "=========================================================\n",
            "Epoch 140\n",
            "training loss: 0.02635193\n",
            "validation loss: 0.74551564\n",
            "=========================================================\n",
            "Epoch 141\n",
            "training loss: 0.02453883\n",
            "validation loss: 0.74760938\n",
            "=========================================================\n",
            "Epoch 142\n",
            "training loss: 0.02658453\n",
            "validation loss: 0.75300545\n",
            "=========================================================\n",
            "Epoch 143\n",
            "training loss: 0.01614922\n",
            "validation loss: 0.74900079\n",
            "=========================================================\n",
            "Epoch 144\n",
            "training loss: 0.05827367\n",
            "validation loss: 0.74890620\n",
            "=========================================================\n",
            "Epoch 145\n",
            "training loss: 0.04436352\n",
            "validation loss: 0.74831760\n",
            "=========================================================\n",
            "Epoch 146\n",
            "training loss: 0.04665836\n",
            "validation loss: 0.75318629\n",
            "=========================================================\n",
            "Epoch 147\n",
            "training loss: 0.04246226\n",
            "validation loss: 0.76142234\n",
            "=========================================================\n",
            "Epoch 148\n",
            "training loss: 0.02241067\n",
            "validation loss: 0.75291955\n",
            "=========================================================\n",
            "Epoch 149\n",
            "training loss: 0.03899118\n",
            "validation loss: 0.74779940\n",
            "=========================================================\n",
            "Epoch 150\n",
            "training loss: 0.01556994\n",
            "validation loss: 0.75341445\n",
            "=========================================================\n",
            "Epoch 151\n",
            "training loss: 0.04021243\n",
            "validation loss: 0.74531657\n",
            "=========================================================\n",
            "Epoch 152\n",
            "training loss: 0.04716276\n",
            "validation loss: 0.74232811\n",
            "=========================================================\n",
            "Epoch 153\n",
            "training loss: 0.04124590\n",
            "validation loss: 0.75265479\n",
            "=========================================================\n",
            "Epoch 154\n",
            "training loss: 0.05046735\n",
            "validation loss: 0.74142349\n",
            "=========================================================\n",
            "Epoch 155\n",
            "training loss: 0.04532096\n",
            "validation loss: 0.73412579\n",
            "=========================================================\n",
            "Epoch 156\n",
            "training loss: 0.03279455\n",
            "validation loss: 0.76319432\n",
            "=========================================================\n",
            "Epoch 157\n",
            "training loss: 0.02860102\n",
            "validation loss: 0.76056021\n",
            "=========================================================\n",
            "Epoch 158\n",
            "training loss: 0.06571799\n",
            "validation loss: 0.75528109\n",
            "=========================================================\n",
            "Epoch 159\n",
            "training loss: 0.02883677\n",
            "validation loss: 0.76342255\n",
            "=========================================================\n",
            "Epoch 160\n",
            "training loss: 0.04479436\n",
            "validation loss: 0.75959051\n",
            "=========================================================\n",
            "Epoch 161\n",
            "training loss: 0.01724813\n",
            "validation loss: 0.76040876\n",
            "=========================================================\n",
            "Epoch 162\n",
            "training loss: 0.05968943\n",
            "validation loss: 0.78979576\n",
            "=========================================================\n",
            "Epoch 163\n",
            "training loss: 0.05015779\n",
            "validation loss: 0.79208517\n",
            "=========================================================\n",
            "Epoch 164\n",
            "training loss: 0.01156758\n",
            "validation loss: 0.78901929\n",
            "=========================================================\n",
            "Epoch 165\n",
            "training loss: 0.03058664\n",
            "validation loss: 0.80092382\n",
            "=========================================================\n",
            "Epoch 166\n",
            "training loss: 0.01695089\n",
            "validation loss: 0.80205542\n",
            "=========================================================\n",
            "Epoch 167\n",
            "training loss: 0.03109398\n",
            "validation loss: 0.77897072\n",
            "=========================================================\n",
            "Epoch 168\n",
            "training loss: 0.01344624\n",
            "validation loss: 0.77955592\n",
            "=========================================================\n",
            "Epoch 169\n",
            "training loss: 0.08604533\n",
            "validation loss: 0.77971792\n",
            "=========================================================\n",
            "Epoch 170\n",
            "training loss: 0.01501428\n",
            "validation loss: 0.77515501\n",
            "=========================================================\n",
            "Epoch 171\n",
            "training loss: 0.02647287\n",
            "validation loss: 0.76941776\n",
            "=========================================================\n",
            "Epoch 172\n",
            "training loss: 0.05196263\n",
            "validation loss: 0.75127447\n",
            "=========================================================\n",
            "Epoch 173\n",
            "training loss: 0.01799742\n",
            "validation loss: 0.75454617\n",
            "=========================================================\n",
            "Epoch 174\n",
            "training loss: 0.01771303\n",
            "validation loss: 0.75471079\n",
            "=========================================================\n",
            "Epoch 175\n",
            "training loss: 0.08345071\n",
            "validation loss: 0.77195406\n",
            "=========================================================\n",
            "Epoch 176\n",
            "training loss: 0.04900600\n",
            "validation loss: 0.78176415\n",
            "=========================================================\n",
            "Epoch 177\n",
            "training loss: 0.01344927\n",
            "validation loss: 0.77996743\n",
            "=========================================================\n",
            "Epoch 178\n",
            "training loss: 0.05130273\n",
            "validation loss: 0.75280225\n",
            "=========================================================\n",
            "Epoch 179\n",
            "training loss: 0.05273348\n",
            "validation loss: 0.74661881\n",
            "=========================================================\n",
            "Epoch 180\n",
            "training loss: 0.01117671\n",
            "validation loss: 0.74225098\n",
            "=========================================================\n",
            "Epoch 181\n",
            "training loss: 0.02845449\n",
            "validation loss: 0.74811614\n",
            "=========================================================\n",
            "Epoch 182\n",
            "training loss: 0.01714122\n",
            "validation loss: 0.75316149\n",
            "=========================================================\n",
            "Epoch 183\n",
            "training loss: 0.01515833\n",
            "validation loss: 0.75744832\n",
            "=========================================================\n",
            "Epoch 184\n",
            "training loss: 0.01988920\n",
            "validation loss: 0.75683004\n",
            "=========================================================\n",
            "Epoch 185\n",
            "training loss: 0.02299189\n",
            "validation loss: 0.77568930\n",
            "=========================================================\n",
            "Epoch 186\n",
            "training loss: 0.02551091\n",
            "validation loss: 0.76738727\n",
            "=========================================================\n",
            "Epoch 187\n",
            "training loss: 0.01325897\n",
            "validation loss: 0.76979381\n",
            "=========================================================\n",
            "Epoch 188\n",
            "training loss: 0.03611180\n",
            "validation loss: 0.76617074\n",
            "=========================================================\n",
            "Epoch 189\n",
            "training loss: 0.01254426\n",
            "validation loss: 0.78029680\n",
            "=========================================================\n",
            "Epoch 190\n",
            "training loss: 0.01434733\n",
            "validation loss: 0.78143591\n",
            "=========================================================\n",
            "Epoch 191\n",
            "training loss: 0.02924291\n",
            "validation loss: 0.77359092\n",
            "=========================================================\n",
            "Epoch 192\n",
            "training loss: 0.03058791\n",
            "validation loss: 0.76449764\n",
            "=========================================================\n",
            "Epoch 193\n",
            "training loss: 0.03684920\n",
            "validation loss: 0.78307796\n",
            "=========================================================\n",
            "Epoch 194\n",
            "training loss: 0.03403412\n",
            "validation loss: 0.76329559\n",
            "=========================================================\n",
            "Epoch 195\n",
            "training loss: 0.01178101\n",
            "validation loss: 0.77223468\n",
            "=========================================================\n",
            "Epoch 196\n",
            "training loss: 0.01886773\n",
            "validation loss: 0.76921523\n",
            "=========================================================\n",
            "Epoch 197\n",
            "training loss: 0.01750262\n",
            "validation loss: 0.76515365\n",
            "=========================================================\n",
            "Epoch 198\n",
            "training loss: 0.02491933\n",
            "validation loss: 0.75892687\n",
            "=========================================================\n",
            "Epoch 199\n",
            "training loss: 0.01842292\n",
            "validation loss: 0.75173217\n",
            "=========================================================\n",
            "Epoch 200\n",
            "training loss: 0.01512970\n",
            "validation loss: 0.76344919\n",
            "=========================================================\n",
            "Epoch 201\n",
            "training loss: 0.03678306\n",
            "validation loss: 0.75203443\n",
            "=========================================================\n",
            "Epoch 202\n",
            "training loss: 0.03763930\n",
            "validation loss: 0.78061718\n",
            "=========================================================\n",
            "Epoch 203\n",
            "training loss: 0.05043467\n",
            "validation loss: 0.76681107\n",
            "=========================================================\n",
            "Epoch 204\n",
            "training loss: 0.01697817\n",
            "validation loss: 0.75601065\n",
            "=========================================================\n",
            "Epoch 205\n",
            "training loss: 0.01333635\n",
            "validation loss: 0.76768810\n",
            "=========================================================\n",
            "Epoch 206\n",
            "training loss: 0.01412777\n",
            "validation loss: 0.77742231\n",
            "=========================================================\n",
            "Epoch 207\n",
            "training loss: 0.05527471\n",
            "validation loss: 0.78523958\n",
            "=========================================================\n",
            "Epoch 208\n",
            "training loss: 0.01052605\n",
            "validation loss: 0.78217280\n",
            "=========================================================\n",
            "Epoch 209\n",
            "training loss: 0.04059015\n",
            "validation loss: 0.76025462\n",
            "=========================================================\n",
            "Epoch 210\n",
            "training loss: 0.03357526\n",
            "validation loss: 0.75494456\n",
            "=========================================================\n",
            "Epoch 211\n",
            "training loss: 0.02242967\n",
            "validation loss: 0.75322682\n",
            "=========================================================\n",
            "Epoch 212\n",
            "training loss: 0.01520735\n",
            "validation loss: 0.75293064\n",
            "=========================================================\n",
            "Epoch 213\n",
            "training loss: 0.03864379\n",
            "validation loss: 0.73989695\n",
            "=========================================================\n",
            "Epoch 214\n",
            "training loss: 0.02027338\n",
            "validation loss: 0.75460523\n",
            "=========================================================\n",
            "Epoch 215\n",
            "training loss: 0.01263272\n",
            "validation loss: 0.75635791\n",
            "=========================================================\n",
            "Epoch 216\n",
            "training loss: 0.00885015\n",
            "validation loss: 0.75616121\n",
            "=========================================================\n",
            "Epoch 217\n",
            "training loss: 0.01129344\n",
            "validation loss: 0.76906276\n",
            "=========================================================\n",
            "Epoch 218\n",
            "training loss: 0.04557569\n",
            "validation loss: 0.77501249\n",
            "=========================================================\n",
            "Epoch 219\n",
            "training loss: 0.00553025\n",
            "validation loss: 0.77895987\n",
            "=========================================================\n",
            "Epoch 220\n",
            "training loss: 0.03224963\n",
            "validation loss: 0.77011633\n",
            "=========================================================\n",
            "Epoch 221\n",
            "training loss: 0.01147407\n",
            "validation loss: 0.77728808\n",
            "=========================================================\n",
            "Epoch 222\n",
            "training loss: 0.05286770\n",
            "validation loss: 0.78948331\n",
            "=========================================================\n",
            "Epoch 223\n",
            "training loss: 0.02551651\n",
            "validation loss: 0.78151941\n",
            "=========================================================\n",
            "Epoch 224\n",
            "training loss: 0.05081427\n",
            "validation loss: 0.77654833\n",
            "=========================================================\n",
            "Epoch 225\n",
            "training loss: 0.01933696\n",
            "validation loss: 0.79026824\n",
            "=========================================================\n",
            "Epoch 226\n",
            "training loss: 0.03377702\n",
            "validation loss: 0.76289499\n",
            "=========================================================\n",
            "Epoch 227\n",
            "training loss: 0.04376786\n",
            "validation loss: 0.77596974\n",
            "=========================================================\n",
            "Epoch 228\n",
            "training loss: 0.01753229\n",
            "validation loss: 0.77336860\n",
            "=========================================================\n",
            "Epoch 229\n",
            "training loss: 0.02920129\n",
            "validation loss: 0.76588368\n",
            "=========================================================\n",
            "Epoch 230\n",
            "training loss: 0.01885501\n",
            "validation loss: 0.77610880\n",
            "=========================================================\n",
            "Epoch 231\n",
            "training loss: 0.07801507\n",
            "validation loss: 0.79018468\n",
            "=========================================================\n",
            "Epoch 232\n",
            "training loss: 0.02423946\n",
            "validation loss: 0.78727078\n",
            "=========================================================\n",
            "Epoch 233\n",
            "training loss: 0.01393624\n",
            "validation loss: 0.77827847\n",
            "=========================================================\n",
            "Epoch 234\n",
            "training loss: 0.02704436\n",
            "validation loss: 0.77797592\n",
            "=========================================================\n",
            "Epoch 235\n",
            "training loss: 0.01214928\n",
            "validation loss: 0.77515239\n",
            "=========================================================\n",
            "Epoch 236\n",
            "training loss: 0.09683678\n",
            "validation loss: 0.62901431\n",
            "=========================================================\n",
            "Epoch 237\n",
            "training loss: 0.03011943\n",
            "validation loss: 0.66012496\n",
            "=========================================================\n",
            "Epoch 238\n",
            "training loss: 0.05298874\n",
            "validation loss: 0.62780946\n",
            "=========================================================\n",
            "Epoch 239\n",
            "training loss: 0.03310667\n",
            "validation loss: 0.61316890\n",
            "=========================================================\n",
            "Epoch 240\n",
            "training loss: 0.03063785\n",
            "validation loss: 0.62354869\n",
            "=========================================================\n",
            "Epoch 241\n",
            "training loss: 0.01774425\n",
            "validation loss: 0.63276446\n",
            "=========================================================\n",
            "Epoch 242\n",
            "training loss: 0.00641551\n",
            "validation loss: 0.63136977\n",
            "=========================================================\n",
            "Epoch 243\n",
            "training loss: 0.01415071\n",
            "validation loss: 0.63201201\n",
            "=========================================================\n",
            "Epoch 244\n",
            "training loss: 0.02936618\n",
            "validation loss: 0.61561525\n",
            "=========================================================\n",
            "Epoch 245\n",
            "training loss: 0.04686235\n",
            "validation loss: 0.61193401\n",
            "=========================================================\n",
            "Epoch 246\n",
            "training loss: 0.01779167\n",
            "validation loss: 0.61611474\n",
            "=========================================================\n",
            "Epoch 247\n",
            "training loss: 0.04084709\n",
            "validation loss: 0.63981837\n",
            "=========================================================\n",
            "Epoch 248\n",
            "training loss: 0.01358448\n",
            "validation loss: 0.63581377\n",
            "=========================================================\n",
            "Epoch 249\n",
            "training loss: 0.01080375\n",
            "validation loss: 0.63623816\n",
            "=========================================================\n",
            "Epoch 250\n",
            "training loss: 0.03334025\n",
            "validation loss: 0.62677777\n",
            "=========================================================\n",
            "Epoch 251\n",
            "training loss: 0.07190702\n",
            "validation loss: 0.60459250\n",
            "=========================================================\n",
            "Epoch 252\n",
            "training loss: 0.04225114\n",
            "validation loss: 0.62132859\n",
            "=========================================================\n",
            "Epoch 253\n",
            "training loss: 0.04083709\n",
            "validation loss: 0.63605362\n",
            "=========================================================\n",
            "Epoch 254\n",
            "training loss: 0.01077124\n",
            "validation loss: 0.64043987\n",
            "=========================================================\n",
            "Epoch 255\n",
            "training loss: 0.03329242\n",
            "validation loss: 0.62533081\n",
            "=========================================================\n",
            "Epoch 256\n",
            "training loss: 0.01077584\n",
            "validation loss: 0.62746370\n",
            "=========================================================\n",
            "Epoch 257\n",
            "training loss: 0.04733479\n",
            "validation loss: 0.60564613\n",
            "=========================================================\n",
            "Epoch 258\n",
            "training loss: 0.02374736\n",
            "validation loss: 0.64227498\n",
            "=========================================================\n",
            "Epoch 259\n",
            "training loss: 0.00696361\n",
            "validation loss: 0.64690900\n",
            "=========================================================\n",
            "Epoch 260\n",
            "training loss: 0.02256984\n",
            "validation loss: 0.63047063\n",
            "=========================================================\n",
            "Epoch 261\n",
            "training loss: 0.03173156\n",
            "validation loss: 0.62279272\n",
            "=========================================================\n",
            "Epoch 262\n",
            "training loss: 0.01591816\n",
            "validation loss: 0.62750053\n",
            "=========================================================\n",
            "Epoch 263\n",
            "training loss: 0.02297481\n",
            "validation loss: 0.64029723\n",
            "=========================================================\n",
            "Epoch 264\n",
            "training loss: 0.02225426\n",
            "validation loss: 0.63878512\n",
            "=========================================================\n",
            "Epoch 265\n",
            "training loss: 0.00520573\n",
            "validation loss: 0.64067942\n",
            "=========================================================\n",
            "Epoch 266\n",
            "training loss: 0.03368555\n",
            "validation loss: 0.63355994\n",
            "=========================================================\n",
            "Epoch 267\n",
            "training loss: 0.02212745\n",
            "validation loss: 0.63928276\n",
            "=========================================================\n",
            "Epoch 268\n",
            "training loss: 0.01011688\n",
            "validation loss: 0.64739972\n",
            "=========================================================\n",
            "Epoch 269\n",
            "training loss: 0.01050455\n",
            "validation loss: 0.64303309\n",
            "=========================================================\n",
            "Epoch 270\n",
            "training loss: 0.03186019\n",
            "validation loss: 0.64506149\n",
            "=========================================================\n",
            "Epoch 271\n",
            "training loss: 0.01244095\n",
            "validation loss: 0.64673281\n",
            "=========================================================\n",
            "Epoch 272\n",
            "training loss: 0.01076483\n",
            "validation loss: 0.64313841\n",
            "=========================================================\n",
            "Epoch 273\n",
            "training loss: 0.01394237\n",
            "validation loss: 0.64133096\n",
            "=========================================================\n",
            "Epoch 274\n",
            "training loss: 0.00762593\n",
            "validation loss: 0.65146625\n",
            "=========================================================\n",
            "Epoch 275\n",
            "training loss: 0.00662796\n",
            "validation loss: 0.65537131\n",
            "=========================================================\n",
            "Epoch 276\n",
            "training loss: 0.01131605\n",
            "validation loss: 0.65506166\n",
            "=========================================================\n",
            "Epoch 277\n",
            "training loss: 0.01223728\n",
            "validation loss: 0.64997822\n",
            "=========================================================\n",
            "Epoch 278\n",
            "training loss: 0.02922375\n",
            "validation loss: 0.66197133\n",
            "=========================================================\n",
            "Epoch 279\n",
            "training loss: 0.03216532\n",
            "validation loss: 0.67193633\n",
            "=========================================================\n",
            "Epoch 280\n",
            "training loss: 0.01297481\n",
            "validation loss: 0.66669309\n",
            "=========================================================\n",
            "Epoch 281\n",
            "training loss: 0.02449653\n",
            "validation loss: 0.64043462\n",
            "=========================================================\n",
            "Epoch 282\n",
            "training loss: 0.07452110\n",
            "validation loss: 0.63686186\n",
            "=========================================================\n",
            "Epoch 283\n",
            "training loss: 0.02483232\n",
            "validation loss: 0.63537306\n",
            "=========================================================\n",
            "Epoch 284\n",
            "training loss: 0.02219071\n",
            "validation loss: 0.64271879\n",
            "=========================================================\n",
            "Epoch 285\n",
            "training loss: 0.01975361\n",
            "validation loss: 0.64030546\n",
            "=========================================================\n",
            "Epoch 286\n",
            "training loss: 0.01124983\n",
            "validation loss: 0.63737792\n",
            "=========================================================\n",
            "Epoch 287\n",
            "training loss: 0.03527181\n",
            "validation loss: 0.65636456\n",
            "=========================================================\n",
            "Epoch 288\n",
            "training loss: 0.02722268\n",
            "validation loss: 0.64707291\n",
            "=========================================================\n",
            "Epoch 289\n",
            "training loss: 0.00585068\n",
            "validation loss: 0.65024596\n",
            "=========================================================\n",
            "Epoch 290\n",
            "training loss: 0.02992071\n",
            "validation loss: 0.63379216\n",
            "=========================================================\n",
            "Epoch 291\n",
            "training loss: 0.02979583\n",
            "validation loss: 0.64155209\n",
            "=========================================================\n",
            "Epoch 292\n",
            "training loss: 0.00910377\n",
            "validation loss: 0.64361984\n",
            "=========================================================\n",
            "Epoch 293\n",
            "training loss: 0.03899695\n",
            "validation loss: 0.64403212\n",
            "=========================================================\n",
            "Epoch 294\n",
            "training loss: 0.01685561\n",
            "validation loss: 0.63654804\n",
            "=========================================================\n",
            "Epoch 295\n",
            "training loss: 0.00705755\n",
            "validation loss: 0.63583547\n",
            "=========================================================\n",
            "Epoch 296\n",
            "training loss: 0.00349464\n",
            "validation loss: 0.63721669\n",
            "=========================================================\n",
            "Epoch 297\n",
            "training loss: 0.01064630\n",
            "validation loss: 0.63632679\n",
            "=========================================================\n",
            "Epoch 298\n",
            "training loss: 0.05588853\n",
            "validation loss: 0.63942075\n",
            "=========================================================\n",
            "Epoch 299\n",
            "training loss: 0.00668038\n",
            "validation loss: 0.64144409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKKlzAS_bDzL"
      },
      "source": [
        "Plotting the Training Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkBQNcgSXYCj",
        "outputId": "fa1b20c2-9c29-46aa-be96-13c2d29ae520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1,epochs+1), train_mean_losses)\n",
        "plt.plot(range(1,epochs+1), valid_mean_losses)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Train and Validation Loss Plot')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMEgzBS-XYCp",
        "outputId": "4fac606f-09e9-4f4a-9962-790acdd0df47"
      },
      "source": [
        "net.load_state_dict(torch.load(\"best_model.pth\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbAZG7VpbQyT"
      },
      "source": [
        "Presenting the Final Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3f8j4JiXYCv",
        "outputId": "fe086490-f75e-4f82-f19f-fcb12486308b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "test_predictions = np.empty((0,3))\n",
        "with torch.no_grad():\n",
        "    for iteration, batch_data in enumerate(test_loader):\n",
        "        X_batch, y_batch = batch_data        \n",
        "        out = net(X_batch)\n",
        "        \n",
        "        test_predictions = np.append(test_predictions, out.numpy(), \n",
        "                                     axis=0)\n",
        "        \n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "test_predictions = np.array(test_predictions)\n",
        "test_predictions = np.argmax(np.array(test_predictions), axis=1)\n",
        "\n",
        "print(\"=========================================================\\n\")\n",
        "print(\"Predicted Class:\")\n",
        "print(test_predictions)\n",
        "print(\"\\nGround Truth:\")\n",
        "print(test_y)\n",
        "\n",
        "print(\"\\n=========================================================\\n\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_y, test_predictions))\n",
        "\n",
        "print(\"\\n=========================================================\\n\")\n",
        "accuracy = accuracy_score(test_y, test_predictions)\n",
        "print(\"Accuracy: {}\".format(accuracy))\n",
        "\n",
        "f1 = f1_score(test_y, test_predictions, average='macro')\n",
        "print(\"F1 Score: \", f1)\n",
        "\n",
        "print(\"\\n=========================================================\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_y, test_predictions))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================\n",
            "\n",
            "Predicted Class:\n",
            "[0 0 1 0 0 2 1 2 1 0 1 1 0 1 2 0 1 1 2 0 0 0 1 0 2 2 2 1 2 2]\n",
            "\n",
            "Ground Truth:\n",
            "[0 0 1 0 0 2 1 2 1 0 1 1 0 1 2 0 1 1 2 0 0 0 1 0 2 2 2 1 2 2]\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Confusion Matrix:\n",
            "[[11  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  0  9]]\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Accuracy: 1.0\n",
            "F1 Score:  1.0\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        11\n",
            "           1       1.00      1.00      1.00        10\n",
            "           2       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXOgBFwDXYC1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}